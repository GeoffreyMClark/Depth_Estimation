# Depth_Estimation
This depth estimation library contains the codebase, working videos, and example images from our work on human-robot symbiotic walking. This library provides a framework for estimating the location of objects in the environment from a single RGB image. The overall objetive is to extract an accurate depth map which is used by other algortihms for estimating human intention and latent biomechanics while walking. To successifully attain live depts-maps from images we provide a custom dataset of 65 scenes with over 50,000 RGB-depth image pairs involving a subject walking over various obstacles and surfaces, including sidewalks, roadways, curbs, gravel, grass, carpeting, and up/down stairs. This took place in differing lighting conditions at a fixed depth range (0.0-6.0 meters). Among the image pairs, 1,723 contain curbs, 5,816 contain uneven floors, and 6,201 were collected while stepping up or down stairs. Therefore, 27.48\% of the dataset involves one of these obstacles. The data collection process also included varied camera positions. Note, depth values is our figures are depicted in a grey color map with darker coloring indicating closer pixels and lighter coloring pixels which are further away. The lowest row of the below figure shows annotated masks used to train the masked-RCNN model to predict foot area and improve temporal consistency. 

![](docs/figures/dataset.png)

## Training
We trained 80\% (52 scenes) of the custom data set with the Adam optimizer, learning rate of $ 10^{-5}$, and withheld 20\% (13 scenes) for testing and validation. The RGB input size is 90x160x3, whereas the output has a single channel for the predicted depth map of 90x160. The ground truth is down-sampled to 3x5, 6x10, 12x20, 23x40, and 45x80 to match the network's intermediate depth feature outputs. Training and evaluation were performed on 8 network architectures, namely the ResNet-50 and EfficientNet encoders with decoders formed with Residual learning, DispNet, and the combination with convolutional layers. All models were trained for 100 epochs and fine-tuned by applying a disparity loss with a learning rate of 10^{-7} for 30 epochs. Additionally we compared the MiDaS v3.0 depth prediction model as well as DPT-Large to our models. Furthermore, we fine-tuned a pre-trained mask RCNN as the masking network using binary cross-entropy loss with masks provided from the custom data set.

The videos below show the predicted depth maps as grayscale where the darker coloring represents closer pixels and lighter that of pixels which are further away. We show here four images of a subject walking over gravel outdoors, carpet indoors, and up/down outdoor stairs. The RBG images on the left are used to directly estimate the depth maps on the right. The videos show that our method of monocular depth prediction is robust to ground type, lighting conditions, and gait.

| | |
|-|-|
| ![](docs/videos/gravel.gif?raw=true) | ![](docs/videos/carpet.gif?raw=true) |
| ![](docs/videos/upstairs.gif?raw=true) | ![](docs/videos/downstairs.gif?raw=true) |